{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gastebois Jacques\n",
    "## Phase 0 : Generation des Trajectoires Expertes (Buffers)\n",
    "\n",
    "Avant de distiller, il nous faut des \"maitres\" a copier. On entraine ici plusieurs reseaux (des ConvNets par defaut) sur le vrai dataset et on enregistre leurs poids a chaque etape.\n",
    "\n",
    "C'est un peu comme enregistrer les differentes prises d'un musicien chez Funky Junk pour pouvoir les analyser plus tard :)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. on prepare le terrain\n",
    "\n",
    "import des modules de base. on verifie bien qu'on a le mps ou la cuda si on est sur une grosse machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as  nn\n",
    "import numpy as  np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "from utils import get_dataset, get_network, get_daparam, TensorDataset, epoch, ParamDiffAug\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    dev = 'mps'\n",
    "else:\n",
    "    dev = 'cpu'\n",
    "    \n",
    "print(f\"ok on va bosser sur {dev} !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. reglages pour les experts\n",
    "\n",
    "on regle combien d'experts on veut et pendant combien de temps ils vont s'entrainer. le papier dit 100 experts sur 50 epochs pour cifar10. c'est long mais c'est le prix de la qualite :p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class buffer_config:\n",
    "    dataset = 'CIFAR10'\n",
    "    model = 'ConvNet'\n",
    "    num_experts = 200\n",
    "    epochs = 50\n",
    "    lr_teacher = 0.01\n",
    "    mom = 0.9\n",
    "    l2 = 5e-4\n",
    "    batch_train = 256\n",
    "    zca = True \n",
    "    data_path = './data'\n",
    "    save_path = './buffers_experts'\n",
    "\n",
    "args =  buffer_config()\n",
    "args.device = dev\n",
    "args.dsa_param = ParamDiffAug()\n",
    "args.dsa_strategy = 'color_crop_cutout_flip_scale_rotate'\n",
    "\n",
    "if  not os.path.exists(args.save_path):\n",
    "    os.makedirs(args.save_path)\n",
    "    print(f\"dossier {args.save_path} cree :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. chargement du dataset reel\n",
    "\n",
    "on recupere les vraies images pour entrainer nos experts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"chargement des donnees...\")\n",
    "channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader, loader_train_dict, class_map, class_map_inv = get_dataset(args.dataset, args.data_path, args.batch_train, '', args=args)\n",
    "\n",
    "print(f\"on a {len(dst_train)} images d'entrainement. c'est parti :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. gros oeuvre : entrainement des experts\n",
    "\n",
    "pour chaque expert, on l'entraine et on garde une trace de ses parametres a chaque epoch. \n",
    "on range ca dans des fichiers .pt pour la suite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experts():\n",
    "    experts_trajectories = []\n",
    "    \n",
    "    for i  in range(args.num_experts):\n",
    "        print(f\"\\\\nentrainement de l'expert nÂ° {i+1}/{args.num_experts}\")\n",
    "        \n",
    "        net = get_network(args.model, channel, num_classes, im_size).to(dev)\n",
    "        net.train()\n",
    "        \n",
    "        optim = torch.optim.SGD(net.parameters(), lr=args.lr_teacher, momentum=args.mom, weight_decay=args.l2)\n",
    "        criterion = nn.CrossEntropyLoss().to(dev)\n",
    "        \n",
    "        trajectoire = []\n",
    "        # on sauve l'etat initial\n",
    "        trajectoire.append([p.detach().cpu().clone() for p in  net.parameters()])\n",
    "        \n",
    "        for e in range(args.epochs):\n",
    "            # une epoch d'entrainement standard\n",
    "            loss, acc = epoch('train', loader_train_dict, net, optim, criterion, args, aug=True)\n",
    "            \n",
    "            # on sauve les poids apres l'epoch\n",
    "            trajectoire.append([p.detach().cpu().clone() for p in net.parameters()])\n",
    "            \n",
    "            if (e+1) % 10 == 0:\n",
    "                print(f\"   epoch {e+1}/{args.epochs} - acc: {acc:.2%}\")\n",
    "        \n",
    "        experts_trajectories.append(trajectoire)\n",
    "        \n",
    "        # on sauve regulierement pour pas tout perdre si ca plante\n",
    "        if (i+1) % 1 == 0:\n",
    "            save_name = os.path.join(args.save_path, f\"replay_buffer_{i}.pt\")\n",
    "            # on sauve juste cet expert dans son fichier\n",
    "            torch.save([trajectoire], save_name)\n",
    "            print(f\"expert {i} sauve dans {save_name} :)\")\n",
    "\n",
    "\n",
    "generate_experts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
